{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Verose/ML_Applications_TAU/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "stYit-uGbayX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import *\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, zero_one_loss\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "import math\n",
        "import os\n",
        "import gzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnh3LsHi1WoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 1: Weighted Random Forest implementation\n",
        "\n",
        "In the first part, you are requested to implement a variation of Random Forest, which we will call \"weighted\" random forest (WRF vs. RF).\n"
      ]
    },
    {
      "metadata": {
        "id": "rUlU2vFt9na4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO 1: As a warm up exercise - \n",
        "\n",
        "We said that the bag size (for calculating out of bag error) of a tree in the RF ensemble is about 1/3 (to be more accurate it's 0.36) of the dataset. Please explain athematically how we got to this number (no need to show a formal proof, an explenation is good enough).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "We wish to calculate the probability that a specific data item *x* is not sampled a tree.\n",
        "\n",
        "We know that every tree samples *n* data items with replacement.\n",
        "\n",
        "The probability that a tree does not sample *x* at a specific sample is (n-1)/n = 1-(1/n)\n",
        "\n",
        "Meaning that the probability of not sampling *x* all is across *n* samples is (1-(1/n))^n\n",
        "\n",
        "Take *n* to infinity get that (1-(1/n))^n -> e^(-1)~0.36\n",
        "\n",
        "Meaning that the bag size of a tree is about 0.36~1/3\n"
      ]
    },
    {
      "metadata": {
        "id": "L7NoZGUq_dw9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 2: implement WRF following the provided class API. You should support both, classification as\n",
        "# well as regression (the type argument can be either \"cat\" or \"reg\"). You should use DecisionTreeClassifier\n",
        "# and DecisionTreeRegressor as the underlying trees.\n",
        "\n",
        "class WRF(BaseEstimator):\n",
        "    def __init__(self, n_trees=100, max_depth=5, n_features=None, weight_type=\"div\"):\n",
        "        '''\n",
        "          init a WRF classifier with the following parameters:\n",
        "\n",
        "          n_trees: the number of trees to use.\n",
        "          max_depth: the depth of each tree (will be passed along to DecisionTreeClassifier/DecisionTreeRegressor).\n",
        "          n_features: the number of features to use for every split. The number should be given to DecisionTreeClassifier/Regressor as max_features.\n",
        "          type: \"cat\" for categorization and \"reg\" for regression.\n",
        "          weight_type: the tree weighting technique. 'div' for 1/error and 'sub' for 1-error.\n",
        "        '''\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.weight_type = weight_type\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "          fit the classifier for the data X with response y.\n",
        "        '''\n",
        "        # <Your Code if needed>\n",
        "        self.trees = []\n",
        "        self.weights = []\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = self.build_tree()\n",
        "            X_tree, y_tree, X_oob, y_oob = self.bootstrap(X, y)\n",
        "            tree.fit(X_tree, y_tree)\n",
        "            weight = self.calculate_weight(tree, X_oob, y_oob)\n",
        "            self.trees.append(tree)\n",
        "            self.weights.append(weight)\n",
        "\n",
        "        # Normalize the weights so they sum to 1\n",
        "        # <Your code goes here>\n",
        "        self.weights = self.softmax(self.weights)\n",
        "\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        return self\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum()\n",
        "\n",
        "    def build_tree(self):\n",
        "        pass\n",
        "\n",
        "    def bootstrap(self, X, y):\n",
        "        '''\n",
        "          This method creates a bootstrap of the dataset (uniformly sample len(X) samples from X with repetitions).\n",
        "          It returns X_tree, y_tree, X_oob, y_oob.\n",
        "          X_tree, y_tree are the bootstrap collections for the given X and y.\n",
        "          X_oob, y_oob are the out of bag remaining instances (the ones that were not sampled as part of the bootstrap)\n",
        "        '''\n",
        "        # <Your code goes here>\n",
        "        if X.size == 0:\n",
        "            return [], [], [], []\n",
        "\n",
        "        length = X.shape[0]\n",
        "        X_tree, Y_tree = [], []\n",
        "\n",
        "        indexes = np.random.choice(range(length), length)\n",
        "        for curr_index in indexes:\n",
        "            X_tree.append(X[curr_index])\n",
        "            Y_tree.append(y[curr_index])\n",
        "\n",
        "        X_oob, Y_oob = [], []\n",
        "        for curr_index in range(length):\n",
        "            if curr_index not in indexes:\n",
        "                X_oob.append(X[curr_index])\n",
        "                Y_oob.append(y[curr_index])\n",
        "\n",
        "        return X_tree, Y_tree, X_oob, Y_oob\n",
        "\n",
        "    def calculate_weight(self, tree, X_oob, y_oob):\n",
        "        '''\n",
        "          This method calculates a weight for the given tree, based on it's performance on\n",
        "          the OOB instances. We support two different types:\n",
        "          if self.weight_type == 'div', we should return 1/error and if it's 'sub' we should\n",
        "          return 1-error. The error is the normalized error rate of the tree on OOB instances.\n",
        "          For classification use 0/1 loss error (i.e., count 1 for every wrong OOB instance and divide by the numbner of OOB instances),\n",
        "          and for regression use mean square error of the OOB instances.\n",
        "        '''\n",
        "\n",
        "        # < Your code goes here>\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "          Predict the label/value of the given instances in the X matrix.\n",
        "          For classification you should implement weighted voting, and for regression implement weighted mean.\n",
        "          Return a list of predictions for the given instances.\n",
        "        '''\n",
        "\n",
        "        # <Your code goes here>\n",
        "        pass\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            \"n_trees\": self.n_trees,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"n_features\": self.n_features,\n",
        "            \"weight_type\": self.weight_type}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "class WRFClassifier(WRF, ClassifierMixin):\n",
        "    def build_tree(self):\n",
        "        return DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.n_features)\n",
        "\n",
        "    def calculate_weight(self, tree, X_oob, y_oob):\n",
        "        predictions = tree.predict(X_oob)\n",
        "        error = zero_one_loss(y_oob, predictions)\n",
        "\n",
        "        if self.weight_type == 'div':\n",
        "            return 1 / (error + 0.0000001)\n",
        "        else:\n",
        "            return 1 - (error + 0.0000001)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions_list = []\n",
        "        for tree, weight in zip(self.trees, self.weights):\n",
        "            if weight > 0:\n",
        "                predictions = tree.predict(X)\n",
        "                predictions_list.append((predictions, weight))\n",
        "        y = []\n",
        "\n",
        "        for ind, _ in enumerate(X):\n",
        "            scores = {}\n",
        "\n",
        "            for preds, weight in predictions_list:\n",
        "                pred = preds[ind]\n",
        "\n",
        "                if pred in scores:\n",
        "                    scores[pred] += weight\n",
        "                else:\n",
        "                    scores[pred] = weight\n",
        "            value = max(scores, key=scores.get)\n",
        "            y += [value]\n",
        "        return y\n",
        "\n",
        "\n",
        "class WRFRegressor(WRF, RegressorMixin):\n",
        "    def build_tree(self):\n",
        "        return DecisionTreeRegressor(max_depth=self.max_depth, max_features=self.n_features)\n",
        "\n",
        "    def calculate_weight(self, tree, X_oob, y_oob):\n",
        "        predictions = tree.predict(X_oob)\n",
        "        error = mean_squared_error(y_oob, predictions)\n",
        "\n",
        "        if self.weight_type == 'div':\n",
        "            return 1 / (error + 0.0000001)\n",
        "        else:\n",
        "            return 1 - (error + 0.0000001)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions_list = []\n",
        "        for tree, weight in zip(self.trees, self.weights):\n",
        "            if weight > 0:\n",
        "                predictions = tree.predict(X)\n",
        "                predictions_list.append((predictions, weight))\n",
        "        y = []\n",
        "\n",
        "        for ind, _ in enumerate(X):\n",
        "            mean = 0\n",
        "\n",
        "            for preds, weight in predictions_list:\n",
        "                mean += preds[ind] * weight\n",
        "            y += [mean]\n",
        "        return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdxAoA7nuTiE",
        "colab_type": "code",
        "outputId": "02929e2f-ae19-4ec6-94ef-bab152d6319c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "cell_type": "code",
      "source": [
        "# prepare the datasets\n",
        "!wget -P data/fashion http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget -P data/fashion http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget -P data/fashion http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget -P data/fashion http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-12 08:46:12--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.74.7\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.74.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [binary/octet-stream]\n",
            "Saving to: ‘data/fashion/train-images-idx3-ubyte.gz.1’\n",
            "\n",
            "train-images-idx3-u 100%[===================>]  25.20M  10.7MB/s    in 2.3s    \n",
            "\n",
            "2018-12-12 08:46:15 (10.7 MB/s) - ‘data/fashion/train-images-idx3-ubyte.gz.1’ saved [26421880/26421880]\n",
            "\n",
            "--2018-12-12 08:46:18--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.35\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29515 (29K) [binary/octet-stream]\n",
            "Saving to: ‘data/fashion/train-labels-idx1-ubyte.gz.1’\n",
            "\n",
            "train-labels-idx1-u 100%[===================>]  28.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-12-12 08:46:18 (201 KB/s) - ‘data/fashion/train-labels-idx1-ubyte.gz.1’ saved [29515/29515]\n",
            "\n",
            "--2018-12-12 08:46:20--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.35\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4422102 (4.2M) [binary/octet-stream]\n",
            "Saving to: ‘data/fashion/t10k-images-idx3-ubyte.gz.1’\n",
            "\n",
            "t10k-images-idx3-ub 100%[===================>]   4.22M  3.68MB/s    in 1.1s    \n",
            "\n",
            "2018-12-12 08:46:22 (3.68 MB/s) - ‘data/fashion/t10k-images-idx3-ubyte.gz.1’ saved [4422102/4422102]\n",
            "\n",
            "--2018-12-12 08:46:23--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.74.66\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.74.66|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5148 (5.0K) [binary/octet-stream]\n",
            "Saving to: ‘data/fashion/t10k-labels-idx1-ubyte.gz.1’\n",
            "\n",
            "t10k-labels-idx1-ub 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-12 08:46:23 (367 MB/s) - ‘data/fashion/t10k-labels-idx1-ubyte.gz.1’ saved [5148/5148]\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AweBS3ty84vI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Section 2: Evaluation\n",
        "\n",
        "In this section you are requested to evaluate your implementation, and compare it with RandomForestClassifier and RandomForestRegressor."
      ]
    },
    {
      "metadata": {
        "id": "Yt0j2Sj5LkTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 3: Implement a tuning method for your classifier. \n",
        "# Note: you could potentially implement WRF as a sklearn classifier and then \n",
        "# simply use GridSearchCV inside. For those of you who wants to take this route, \n",
        "# you are welcome to modify the implementation of WRF accordingly. Check out: https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator\n",
        "\n",
        "def tune(classifier, X, y, scoring, arguments, cv=5):\n",
        "    '''\n",
        "      This method is doing exactly what GridSearchCV is doing for a sklearn classifier.\n",
        "      It will run cross validation training with cv folds many times. Each time it will evaluate the CV \"performance\" on a different\n",
        "      combination of the given arguments. You should check every combination of the given arguments and return a dictionary with\n",
        "      the best argument combination. For classification, \"performance\" is accuracy. For Regression, \"performance\" is mean square error.\n",
        "\n",
        "      classifier: it's the WRF classifier to tune\n",
        "      X, y: the dataset to tune over\n",
        "      arguments: a dictionary with keys are one of n_trees, max_depth, n_features, weight_type\n",
        "      and the values are lists of values to test for each argument (see more in GridSearchCV)\n",
        "    '''\n",
        "  \n",
        "    # <Your code goes here>\n",
        "    clf = GridSearchCV(classifier, arguments, cv=cv, scoring=scoring)\n",
        "    result = clf.fit(X, y)\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RquZ6HHroPQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
        "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def load_mnist_train_test():\n",
        "    X_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "    X_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "    X_train = X_train[:7000]\n",
        "    y_train = y_train[:7000]\n",
        "    X_test = X_test[:5000]\n",
        "    y_test = y_test[:5000]\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def load_housing():\n",
        "    data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ML course/data/housing.csv')\n",
        "    \n",
        "    # data preparation\n",
        "    ocean_prox = pd.get_dummies(data['ocean_proximity'], prefix='OCEAN')\n",
        "    data = pd.concat([data, ocean_prox], axis=1)\n",
        "    y = data[\"median_house_value\"]\n",
        "    X = data.drop(columns=[\"ocean_proximity\", \"median_house_value\"])\n",
        "    X = X.apply(lambda x: x.fillna(x.mean()))\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "    return X_train.values, X_test.values, y_train.values, y_test.values\n",
        "\n",
        "\n",
        "def tune_predict(X_test, X_train, y_test, y_train, rf, wrf):\n",
        "    rf_parameters = {\n",
        "        'n_estimators': (50, 100, 120),\n",
        "        'max_depth': (10, 20, 30),\n",
        "        'max_features': (1, 'sqrt'),\n",
        "    }\n",
        "    wrf_parameters = {\n",
        "        'n_trees': (50, 100, 120),\n",
        "        'max_depth': (10, 20, 30),\n",
        "        'n_features': (1, 'sqrt'),\n",
        "        'weight_type': ('div', 'sub'),\n",
        "    }\n",
        "    tune_rf = tune(rf, X_train, y_train, 'accuracy', rf_parameters)\n",
        "    tune_wrf = tune(wrf, X_train, y_train, 'accuracy', wrf_parameters)\n",
        "    rf = tune_rf.best_estimator_\n",
        "    wrf = tune_wrf.best_estimator_\n",
        "    rf_predict = rf.predict(X_test)\n",
        "    wrf_predict = wrf.predict(X_test)\n",
        "    return rf_predict, wrf_predict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2nuDmPrXNjeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 4: Evaluate your implementation and compare it RandomForestClassifier/Regressor provided by sklearn.\n",
        "\n",
        "# For classification use the Fashion MNIST, but subsample the dataset to contain only 7K instances (out of the 60K available instances, you may simply select the first 7K instance from the data).\n",
        "# - Tune both classifiers (WRF and RandomForestClassifier) before evaluation.\n",
        "# - Evaluate both classifiers on the first 5K instances from the provided test data.\n",
        "# - Report accuracy and a full confusion matrix for each classifier.\n",
        "\n",
        "# For regression use the California housing dataset from Kaggle that we used in class:\n",
        "# https://www.kaggle.com/harrywang/housing#housing.csv\n",
        "\n",
        "# - Split the dataset to train and test (test_size=0.1, random_state=0)\n",
        "# - Tune both regressors (WRF and RandomForestRegressor) before evaluation on the training set.\n",
        "# - Evaluate both regressors on the test set.\n",
        "# - Report mean square error.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvp-eVUHoIJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tune_predict(X_test, X_train, y_test, y_train, rf, wrf, scoring, rf_parameters, wrf_parameters):\n",
        "    tune_rf = tune(rf, X_train, y_train, scoring, rf_parameters)\n",
        "    tune_wrf = tune(wrf, X_train, y_train, scoring, wrf_parameters)\n",
        "    print('GridSearch RF best params: {}'.format(tune_rf.best_params_))\n",
        "    print('GridSearch WRF best params: {}'.format(tune_wrf.best_params_))\n",
        "    rf = tune_rf.best_estimator_\n",
        "    wrf = tune_wrf.best_estimator_\n",
        "    rf_predict = rf.predict(X_test)\n",
        "    wrf_predict = wrf.predict(X_test)\n",
        "    return rf_predict, wrf_predict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pvTy251e_eEK",
        "colab_type": "code",
        "outputId": "db09add6-1657-4ce5-b2fa-7cf5d864b29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "cell_type": "code",
      "source": [
        "# classifiers\n",
        "X_train, X_test, y_train, y_test = load_mnist_train_test()\n",
        "rf = RandomForestClassifier()\n",
        "wrf = WRFClassifier()\n",
        "rf_parameters = {\n",
        "    'n_estimators': (100, 120),\n",
        "    'max_depth': (10, 20, 25),\n",
        "    'max_features': ('sqrt',),\n",
        "}\n",
        "wrf_parameters = {\n",
        "    'n_trees': (100, 120),\n",
        "    'max_depth': (10, 20, 25),\n",
        "    'n_features': ('sqrt',),\n",
        "    'weight_type': ('div', 'sub'),\n",
        "}\n",
        "rf_predict, wrf_predict = tune_predict(X_test, X_train, y_test, y_train, rf, wrf, 'accuracy', rf_parameters, wrf_parameters)\n",
        "# Evaluation\n",
        "print(\"Accuracy RandomForestClassifier\", accuracy_score(y_test, rf_predict))\n",
        "print(\"Confusion Matrix RandomForestClassifier\", confusion_matrix(y_test, rf_predict))\n",
        "print(\"Accuracy WeightedRandomForestClassifier\", accuracy_score(y_test, wrf_predict))\n",
        "print(\"Confusion Matrix WeightedRandomForestClassifier\", confusion_matrix(y_test, wrf_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GridSearch RF best params: {'max_depth': 25, 'max_features': 'sqrt', 'n_estimators': 120}\n",
            "GridSearch WRF best params: {'max_depth': 20, 'n_features': 'sqrt', 'n_trees': 100, 'weight_type': 'sub'}\n",
            "Accuracy RandomForestClassifier 0.8488\n",
            "Confusion Matrix RandomForestClassifier [[404   2  14  25   2   1  53   0   6   0]\n",
            " [  1 459   3  17   1   0   0   0   0   0]\n",
            " [  5   0 403   5  75   0  32   0   1   0]\n",
            " [  8   4   5 444  18   0  20   0   1   0]\n",
            " [  0   2  73  26 393   1  25   0   1   0]\n",
            " [  0   0   0   0   0 457   0  21   0   7]\n",
            " [ 66   2  62  20  41   0 283   0   8   0]\n",
            " [  0   0   0   0   0  11   0 451   0  38]\n",
            " [  0   3   5   2   3   2   6   2 502   1]\n",
            " [  0   0   0   0   0   7   1  20   1 448]]\n",
            "Accuracy WeightedRandomForestClassifier 0.8428\n",
            "Confusion Matrix WeightedRandomForestClassifier [[407   2   8  23   3   2  56   0   6   0]\n",
            " [  0 460   3  16   1   0   1   0   0   0]\n",
            " [  6   0 397   5  76   0  35   0   2   0]\n",
            " [ 10   4   3 438  21   0  22   0   2   0]\n",
            " [  0   1  69  24 398   0  27   0   2   0]\n",
            " [  0   0   0   0   0 453   0  23   2   7]\n",
            " [ 77   1  61  18  52   0 262   0  11   0]\n",
            " [  0   0   0   0   0  10   0 448   0  42]\n",
            " [  0   1   4   4   3   3   7   2 501   1]\n",
            " [  0   0   0   0   0   6   1  19   1 450]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SK9ENBMCF5Hh",
        "colab_type": "code",
        "outputId": "5a0b8a93-8ff3-4daf-906e-d233dfc0efb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# regressors\n",
        "X_train, X_test, y_train, y_test = load_housing()\n",
        "rf = RandomForestRegressor()\n",
        "wrf = WRFRegressor()\n",
        "rf_parameters = {\n",
        "    'n_estimators': (5, 10),\n",
        "    'max_depth': (3, 5, 8),\n",
        "    'max_features': (1, 5, 'sqrt'),\n",
        "}\n",
        "wrf_parameters = {\n",
        "    'n_trees': (5, 10),\n",
        "    'max_depth': (3, 5, 8),\n",
        "    'n_features': (1, 5, 'sqrt'),\n",
        "    'weight_type': ('div', 'sub'),\n",
        "}\n",
        "rf_predict, wrf_predict = tune_predict(X_test, X_train, y_test, y_train, rf, wrf, 'neg_mean_squared_error', rf_parameters, wrf_parameters)\n",
        "# Evaluation\n",
        "print(\"Mean Squared Error RandomForestRegressor\", mean_squared_error(y_test, rf_predict))\n",
        "print(\"Mean Squared Error WeightedRandomForestRegressor\", mean_squared_error(y_test, wrf_predict))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GridSearch RF best params: {'max_depth': 8, 'max_features': 5, 'n_estimators': 10}\n",
            "GridSearch WRF best params: {'max_depth': 8, 'n_features': 5, 'n_trees': 10, 'weight_type': 'div'}\n",
            "Mean Squared Error RandomForestRegressor 3316573275.093102\n",
            "Mean Squared Error WeightedRandomForestRegressor 3223331713.7324686\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}